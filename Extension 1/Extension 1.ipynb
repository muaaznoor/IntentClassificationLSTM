{"cells":[{"cell_type":"markdown","metadata":{"id":"-4WVrNKQC6nQ"},"source":["## Import Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6647,"status":"ok","timestamp":1671591895787,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"Rb-WLp5Z-cdy","outputId":"42cab185-8446-44d1-fd53-670917fde3bc"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["from __future__ import division\n","import os, sys, re, json, time, datetime, shutil\n","import itertools, collections\n","from importlib import reload\n","\n","import random \n","import numpy as np\n","import pandas as pd\n","import os\n","import sys\n","import matplotlib.pyplot as plt\n","from numpy.linalg import *\n","np.random.seed(42)  # don't change this line\n","\n","import base64\n","\n","# NLTK, NumPy, and Pandas.\n","import nltk\n","nltk.download('punkt')\n","from nltk.tree import Tree\n","from numpy import random as rd\n","from nltk.tokenize import word_tokenize\n","import random\n","\n","import collections\n","import re\n","import time\n","import itertools\n","from collections import defaultdict, Counter\n","\n","import glob\n","from argparse import ArgumentParser\n","\n","#Pytorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","metadata":{"id":"ZBlsjn0KncxL"},"source":["## Load [Datasets](https://huggingface.co/datasets/SetFit/amazon_massive_intent_en-US)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22436,"status":"ok","timestamp":1671591918217,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"9m4SGpN0nggy","outputId":"81b467e2-6e31-4a0c-eec3-d9a248ca28c1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":1486,"status":"ok","timestamp":1671591919699,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"CmUN4g-VnrVj"},"outputs":[],"source":["df_train = pd.read_json(\"/content/drive/Shareddrives/CIS-5300_Final-Project/train.jsonl\", lines=True)\n","# df_train.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":380,"status":"ok","timestamp":1671591920075,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"ikNFPOY8n-mm"},"outputs":[],"source":["df_validation = pd.read_json(\"/content/drive/Shareddrives/CIS-5300_Final-Project/validation.jsonl\", lines=True)\n","# df_validation.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":536,"status":"ok","timestamp":1671591920609,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"ZGmrNL2Kn_uT"},"outputs":[],"source":["df_test = pd.read_json(\"/content/drive/Shareddrives/CIS-5300_Final-Project/test.jsonl\", lines=True)\n","# df_test.shape"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1671591920610,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"N14EehqeSM69"},"outputs":[],"source":["# use prefix of (child) label text as parent label\n","df_train[\"parent_label_text\"] = df_train[\"label_text\"].apply(lambda x: x.split('_')[0])\n","# factorize to get integer value for each parent label\n","df_train[\"parent_label\"] = pd.factorize(df_train[\"parent_label_text\"])[0]"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1671591920610,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"Su6oJdWhgLtG"},"outputs":[],"source":["# create dictionary indexer for parent class text/label\n","parent_label_idx = dict(zip(df_train[\"parent_label_text\"],df_train[\"parent_label\"]))"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1671591920610,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"vvUaQR8yfelz"},"outputs":[],"source":["# get parent class text/label for validation data\n","df_validation[\"parent_label_text\"] = df_validation[\"label_text\"].apply(lambda x: x.split('_')[0])\n","df_validation[\"parent_label\"] = df_validation[\"parent_label_text\"].apply(lambda x: parent_label_idx[x])"]},{"cell_type":"markdown","metadata":{"id":"k_ZyO9a9QwKg"},"source":["## Preprocessing"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":1865,"status":"ok","timestamp":1671591922471,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"2AsW9_eWTtU6"},"outputs":[],"source":["df_all = pd.concat([df_train, df_validation, df_test]).reset_index()\n","\n","tokenized_data = [word_tokenize(df_all['text'][i]) for i in range(len(df_all['text']))]\n","\n","vocab = {word for sentence in tokenized_data for word in sentence}\n","vocab.add('\u003cPAD\u003e')\n","\n","word_to_idx = { w : i for i, w in enumerate(vocab) }"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1671591922471,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"fIOXBFqUr1XD"},"outputs":[],"source":["def pre_process(data, word_to_idx):\n","  tokenized_data = [word_tokenize(data['text'][i]) for i in range(len(data))]\n","\n","  lens = np.array([len(sentence) for sentence in tokenized_data])\n","  \n","  tokens = [word_to_idx[word] for sentence in tokenized_data for word in sentence]\n","\n","  # add one extra \u003cPAD\u003e token at the end of each sequence\n","  padded_tokens = np.full([len(tokenized_data), max(lens) + 1], word_to_idx['\u003cPAD\u003e'])\n","  for i in range(len(tokenized_data)):\n","    for j in range(len(tokenized_data[i])):\n","      padded_tokens[i][j] = word_to_idx[tokenized_data[i][j]]\n","\n","  labels = np.array(data['label'])\n","  parent_labels = np.array(data['parent_label'])\n","    \n","  return padded_tokens, lens, labels, parent_labels"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1671591922472,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"gukyScOlS6Ri"},"outputs":[],"source":["glove_file = \"glove.840B.300d.txt\""]},{"cell_type":"markdown","metadata":{"id":"EQvv34NcE6BM"},"source":["# Glove Embeddings"]},{"cell_type":"markdown","metadata":{"id":"Q5ulmPzRwr3T"},"source":["## Download [Glove Embeddings](https://nlp.stanford.edu/projects/glove/) \n"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36590,"status":"ok","timestamp":1671591997261,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"nT27aeXBVw01"},"outputs":[{"name":"stdout","output_type":"stream","text":["Archive:  /content/drive/Shareddrives/CIS-5300_Final-Project/glove.840B.300d.zip\n","  inflating: glove.840B.300d.txt     \n","total 5513928\n","drwxr-xr-x 1 root root       4096 Dec 21 03:05 .\n","drwx------ 6 root root       4096 Dec 21 03:05 drive\n","drwxr-xr-x 1 root root       4096 Dec 21 03:03 ..\n","drwxr-xr-x 1 root root       4096 Dec 19 14:31 sample_data\n","drwxr-xr-x 4 root root       4096 Dec 19 14:30 .config\n","-rw-rw-r-- 1 root root 5646236541 Oct 24  2015 glove.840B.300d.txt\n"]}],"source":["#this takes about 10 minutes to run\n","#!wget -nc https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n","!unzip /content/drive/Shareddrives/CIS-5300_Final-Project/glove.840B.300d.zip\n","!ls -lat"]},{"cell_type":"markdown","metadata":{"id":"JFe9SkOcYugK"},"source":["## Get Glove Embeddings"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"background_save":true},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1671591997262,"user":{"displayName":"Kevin Xie","userId":"18383625465087567440"},"user_tz":300},"id":"4RNiORwiYTzi"},"outputs":[],"source":["def get_glove_mapping(vocab, file):\n","    \"\"\"\n","    Gets the mapping of words from the vocabulary to pretrained embeddings\n","    \n","    INPUT:\n","    vocab       - set of vocabulary words\n","    file        - file with pretrained embeddings\n","\n","    OUTPUT:\n","    glove_map   - mapping of words in the vocabulary to the pretrained embedding\n","    \n","    \"\"\"\n","    \n","    glove_map = {}\n","    with open(file,'rb') as fi:\n","        for l in fi:\n","            try:\n","                emd_lst = l.decode().split(' ')\n","                word = emd_lst.pop(0)\n","                emd_lst = [float(n) for n in emd_lst]\n","\n","                if word in vocab:\n","                  glove_map[word] = np.array(emd_lst)\n","            except:\n","                #some lines have urls, we don't need them.\n","                pass\n","    return glove_map"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"kYettRoymWTz"},"outputs":[],"source":["glove_map = get_glove_mapping(vocab,glove_file)"]},{"cell_type":"markdown","metadata":{"id":"HoZuTVB9mcZb"},"source":["## Get Embedding Matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"djhLc9X_Z2E0"},"outputs":[],"source":["def get_dimensions():\n","    d_out =  60 #number of outputs\n","    n_embed =  len(vocab) #size of the dictionary of embeddings\n","    d_embed =  300 # the size of each embedding vector\n","    return d_out, n_embed, d_embed\n","d_out,n_embed,d_embed = get_dimensions()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"MMIZjt4HYuJB"},"outputs":[],"source":["def get_embedding_matrix(n_embed, d_embed, glove_map):\n","    \"\"\"\n","    Initialize the weight matrix\n","    \n","    INPUT:\n","    n_embed         - size of the dictionary of embeddings\n","    d_embed         - the size of each embedding vector\n","\n","    OUTPUT:\n","    embedding_matrix  - matrix of mapping from word id to embedding \n","    \"\"\"\n","    train_words = vocab\n","    embedding_matrix = np.full((n_embed, d_embed), np.random.normal())\n","\n","    for i, word in enumerate(train_words):\n","\n","        if word in glove_map.keys():\n","\n","            embedding_matrix[i] = glove_map[word]\n","    \n","    return embedding_matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"rotHEiIqm2sV"},"outputs":[],"source":["embedding_matrix = get_embedding_matrix(n_embed, d_embed, glove_map)\n","embedding_data = (embedding_matrix.shape, embedding_matrix[:155])"]},{"cell_type":"markdown","metadata":{"id":"FI5HX8dGNQyR"},"source":["# Define Dataloader "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"tOubHTzb8CS2"},"outputs":[],"source":["class SSTpytorchDataset(Dataset):\n","    def __init__(self, dataset, tokens, word_to_idx, word_dropout = 0.3, split='train'):\n","        super(SSTpytorchDataset, self).__init__()\n","        assert split in ['train', 'test', 'dev'], \"Error!\"\n","        self.ds = dataset\n","        self.split = split\n","        self.word_to_idx = word_to_idx\n","        #self.word_dropout = word_dropout\n","        self.data_x, self.data_ns, self.data_y, self.data_y_parent = pre_process(dataset, self.word_to_idx)\n","\n","    def __len__(self):\n","        return self.data_x.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        return self.data_x[idx], self.data_ns[idx], self.data_y[idx], self.data_y_parent[idx]\n","        "]},{"cell_type":"markdown","metadata":{"id":"fwthJbWgFNxh"},"source":["# Modeling"]},{"cell_type":"markdown","metadata":{"id":"Z6rGLI6jm-0g"},"source":["## Define Embedding Layer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"4XGB1pAabQAf"},"outputs":[],"source":["def create_emb_layer(embedding_matrix, non_trainable=False):\n","    \"\"\"\n","    Create the embedding layer\n","    \n","    INPUT:\n","    embedding_matrix  - matrix of mapping from word id to embedding\n","    non_trainable   - Flag for whether the weight matrix should be trained. \n","                      If it is set to True, don't update the gradients\n","\n","    OUTPUT:\n","    emb_layer       - embedding layer \n","    \n","    \"\"\"\n","\n","    emb_layer = nn.Embedding.from_pretrained(torch.Tensor(embedding_matrix), padding_idx=word_to_idx['\u003cPAD\u003e'])\n","\n","    return emb_layer"]},{"cell_type":"markdown","metadata":{"id":"nnhB6TKJprvY"},"source":["## Define Train \u0026 Evaluate Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"iMPfgoTym0Yj"},"outputs":[],"source":["def train(model, word_to_idx, lr = .005, drop_out = 0, word_dropout = .3, batch_size = 16, weight_decay = 1e-5, model_type= \"LSTM\"):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    trainset = SSTpytorchDataset(df_train, word_dropout, word_to_idx, 'train')\n","    # testset = SSTpytorchDataset(df_test, word_dropout, word_to_idx, 'test')\n","    devset = SSTpytorchDataset(df_validation, word_dropout, word_to_idx, 'dev')\n","\n","    train_iter = DataLoader(trainset, batch_size, shuffle=True, num_workers=0)\n","    # test_iter = DataLoader(testset, batch_size, shuffle=False, num_workers=0)\n","    dev_iter = DataLoader(devset, batch_size, shuffle=False, num_workers=0)\n","    \n","    model = model\n","    model.to(device)\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay = weight_decay)\n","    acc, val_loss, _ = evaluate(dev_iter, model, device, model_type)\n","    best_acc = acc\n","\n","    print(\n","        'epoch |   %        |  loss  |  avg   |val loss|   acc   |  best  | time | save |')\n","    print(\n","        'val   |            |        |        | {:.4f} | {:.4f} | {:.4f} |      |      |'.format(\n","            val_loss, acc, best_acc))\n","\n","    iterations = 0\n","    last_val_iter = 0\n","    train_loss = 0\n","    start = time.time()\n","    _save_ckp = ''\n","    for epoch in range(epochs):\n","        \n","        n_correct, n_total, train_loss = 0, 0, 0\n","        last_val_iter = 0\n","        \n","        for batch_idx, batch in enumerate(train_iter):\n","            # switch model to training mode, clear gradient accumulators\n","            \n","            model.train()\n","            optimizer.zero_grad()\n","            iterations += 1\n","\n","            data, lens, child_label, parent_label = batch\n","            data = data.to(device)\n","\n","            # get prediction for parent and child class labels\n","            parent, child = model(data, lens)\n","\n","            # separate loss computation for parent/child class label\n","            loss_parent = criterion(parent, parent_label)\n","            loss_child = criterion(child, child_label)\n","\n","            # sum loss for parent and child class labels to optimize over both predictions\n","            (loss_parent+loss_child).backward()\n","            optimizer.step()\n","\n","            loss = loss_parent.item() + loss_child.item()\n","\n","            train_loss += loss\n","            print('\\r {:4d} | {:4d}/{} | {:.4f} | {:.4f} |'.format(\n","                epoch, batch_size * (batch_idx + 1), len(trainset), loss,\n","                       train_loss / (iterations - last_val_iter)), end='')\n","\n","            if iterations \u003e 0 and iterations % dev_every == 0:\n","                acc, val_loss, _ = evaluate(dev_iter, model, device, model_type)\n","                if acc \u003e best_acc:\n","                    best_acc = acc\n","                    torch.save(model.state_dict(), save_path)\n","                    _save_ckp = '*'\n","\n","                print(\n","                    ' {:.4f} | {:.4f} | {:.4f} | {:.2f} | {:4s} |'.format(\n","                        val_loss, acc, best_acc, (time.time() - start) / 60,\n","                        _save_ckp))\n","\n","                train_loss = 0\n","                last_val_iter = iterations\n","    model.load_state_dict(torch.load(save_path)) #this will be the best model\n","    test_y_pred = evaluate(dev_iter, model, device, model_type, \"test\")\n","    print(\"\\nValidation Accuracy : \", evaluate(dev_iter,model, device, model_type))\n","    return best_acc, test_y_pred\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"YnMrMbWcINMi"},"outputs":[],"source":["from sklearn.metrics import f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"qKen8d8knAj8"},"outputs":[],"source":["def evaluate(loader, model, device, model_type = \"LSTM\", split = \"dev\"):\n","    model.eval()\n","    n_correct, n = 0, 0\n","    losses = []\n","    y_pred = []\n","    labels = []\n","    with torch.no_grad():\n","        for batch_idx, batch in enumerate(loader):\n","            data, lens, label, _ = batch\n","            data = data.to(device)\n","            label = label.to(device).long()\n","            \n","            parent, answer = model(data, lens)\n","            if split != \"test\":\n","                n_correct += (torch.max(answer, 1)[1].view(label.size()) == label).sum().item()\n","                n += answer.shape[0]\n","                loss = criterion(answer, label)\n","                losses.append(loss.data.cpu().numpy())\n","                y_pred.extend(torch.max(answer, 1)[1].view(label.size()).tolist())\n","                labels.extend(label.tolist())\n","            else:\n","                y_pred.extend(torch.max(answer, 1)[1].view(label.size()).tolist())\n","                labels.extend(label.tolist())\n","    if split != \"test\":\n","        acc = 100. * n_correct / n\n","        loss = np.mean(losses)\n","        # print(labels)\n","        # print(y_pred)\n","        f1 = f1_score(labels, y_pred, average=\"weighted\")\n","        return acc, loss, f1\n","    else:\n","        return y_pred\n"]},{"cell_type":"markdown","metadata":{"id":"AAflZLIpNybW"},"source":["## Define LSTM Network"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"1q7oqyrCP8fK"},"outputs":[],"source":["#@title\n","import random as random\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch.nn import LSTM, GRU\n","\n","class LSTM_Classifier(nn.Module):\n","\n","    def __init__(self,\n","                 n_embed=20000,\n","                 d_embed=300,\n","                 d_hidden=150,\n","                 d_out=60,\n","                 embeddings=None,\n","                 nl = 1,\n","                 bidirectional = True,\n","                 gru = False\n","                 ):\n","        super(LSTM_Classifier, self).__init__()\n","\n","        self.d_hidden = d_hidden\n","        self.bidrectional = bidirectional\n","        self.num_layers = nl\n","\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.embed = create_emb_layer(embedding_matrix,False)\n","\n","        self.lstm = nn.LSTM(d_embed, self.d_hidden, self.num_layers, batch_first=True,\n","                              bidirectional=bidirectional)\n","        \n","        self.fc_out_parent = nn.Linear(self.d_hidden*2, 18)\n","        self.fc_out = nn.Linear(self.d_hidden*2, d_out)\n","        self.dropout = nn.Dropout(p=0.2)\n","\n","    def forward(self, text, seq_lengths):\n","\n","        # parent class label\n","\n","        # do not include final pad token\n","        x_parent = self.embed(text[:,:-1])\n","        x_parent = pack_padded_sequence(x_parent, seq_lengths, batch_first=True, enforce_sorted=False)    \n","        x_parent, hidden = self.lstm(x_parent)\n","        x_parent, _ = pad_packed_sequence(x_parent, batch_first=True)\n","        x_parent = self.fc_out_parent(x_parent)\n","        x_parent, _ = torch.max(x_parent, 1)\n","\n","        # child class label\n","\n","        # only include final pad token\n","        x = self.embed(text[:,-1])\n","        # input final hidden state from parent class label lstm\n","        x, _ = self.lstm(x.unsqueeze(1), hidden)\n","        x = self.fc_out(x)\n","        x, _ = torch.max(x, 1)\n","\n","        return x_parent, x"]},{"cell_type":"markdown","metadata":{"id":"JmdzEvd2Nc2M"},"source":["# Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x7FkX-MDokS7"},"outputs":[],"source":["torch.manual_seed(1234)\n","criterion = nn.CrossEntropyLoss()\n","batch_size = 128\n","epochs = 10\n","dev_every = 100\n","lr = 0.01\n","save_path = \"best_model\"\n","drop_out = 0\n","word_dropout = 0\n","weight_decay = 0\n","\n","model = LSTM_Classifier(n_embed=n_embed, d_embed=d_embed, d_hidden=150, d_out=d_out, bidirectional=True)\n","dev_value, test_y_pred = train(model, word_to_idx, lr, drop_out, word_dropout, batch_size, weight_decay, \"LSTM\") "]}],"metadata":{"colab":{"collapsed_sections":["-4WVrNKQC6nQ","ZBlsjn0KncxL","k_ZyO9a9QwKg","EQvv34NcE6BM","Q5ulmPzRwr3T","FI5HX8dGNQyR"],"name":"","provenance":[{"file_id":"1-gHzyDS9llGBQs-uhgPZowubGE1Gbssf","timestamp":1670612728861},{"file_id":"1NtblqGYiClITktOKVNTMH5_YGjjNpq_j","timestamp":1670097881819},{"file_id":"17N0rK0m_b8quTu2j_N-HW2AGWEvkHZD4","timestamp":1666564593460},{"file_id":"13xUhf4GEXjXym5_S3Bbg3Ftg0wdIQC8F","timestamp":1665861302092},{"file_id":"1-ueNZYNsPx-ETZbRhGwPB-0AgVmstY0B","timestamp":1665691607811},{"file_id":"1VmAGfOn2wyK6SPcFKhEAbxsgKjrLysb3","timestamp":1665691364654}],"toc_visible":true,"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}