{"cells":[{"cell_type":"markdown","metadata":{"id":"-4WVrNKQC6nQ"},"source":["## Import Packages"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14884,"status":"ok","timestamp":1670619329004,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"},"user_tz":300},"id":"Rb-WLp5Z-cdy","outputId":"73325426-c187-411b-fe3f-c86859d15a2f"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"]}],"source":["from __future__ import division\n","import os, sys, re, json, time, datetime, shutil\n","import itertools, collections\n","from importlib import reload\n","\n","import random \n","import numpy as np\n","import pandas as pd\n","import os\n","import sys\n","import matplotlib.pyplot as plt\n","from numpy.linalg import *\n","np.random.seed(42)  # don't change this line\n","\n","import base64\n","\n","# NLTK, NumPy, and Pandas.\n","import nltk\n","nltk.download('punkt')\n","from nltk.tree import Tree\n","from numpy import random as rd\n","from nltk.tokenize import word_tokenize\n","import random\n","\n","import collections\n","import re\n","import time\n","import itertools\n","from collections import defaultdict, Counter\n","\n","import glob\n","from argparse import ArgumentParser\n","\n","#Pytorch\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"markdown","metadata":{"id":"ZBlsjn0KncxL"},"source":["## Load [Datasets](https://huggingface.co/datasets/SetFit/amazon_massive_intent_en-US)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20967,"status":"ok","timestamp":1670619349966,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"},"user_tz":300},"id":"9m4SGpN0nggy","outputId":"8d0d8a28-5f21-4974-c691-31878ba2346f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":900,"status":"ok","timestamp":1670619350858,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"},"user_tz":300},"id":"CmUN4g-VnrVj","outputId":"e6f41a9d-22fd-4e28-b5f5-c4de9743b4c4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(11514, 4)"]},"metadata":{},"execution_count":3}],"source":["df_train = pd.read_json(\"/content/drive/Shareddrives/CIS-5300_Final-Project/train.jsonl\", lines=True)\n","df_train.shape"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":412,"status":"ok","timestamp":1670619351269,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"},"user_tz":300},"id":"ikNFPOY8n-mm","outputId":"507f7ede-ac4e-4921-92c8-f4ea900e7032"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2033, 4)"]},"metadata":{},"execution_count":4}],"source":["df_validation = pd.read_json(\"/content/drive/Shareddrives/CIS-5300_Final-Project/validation.jsonl\", lines=True)\n","df_validation.shape"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":290,"status":"ok","timestamp":1670619351558,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"},"user_tz":300},"id":"ZGmrNL2Kn_uT","outputId":"54f7e35d-5742-4a25-df9a-8311a78b48ac"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(2974, 4)"]},"metadata":{},"execution_count":5}],"source":["df_test = pd.read_json(\"/content/drive/Shareddrives/CIS-5300_Final-Project/test.jsonl\", lines=True)\n","df_test.shape"]},{"cell_type":"markdown","metadata":{"id":"Q5ulmPzRwr3T"},"source":["## Download [Glove Embeddings](https://nlp.stanford.edu/projects/glove/) \n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":78750,"status":"ok","timestamp":1670619430306,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"},"user_tz":300},"id":"nT27aeXBVw01","outputId":"5fd025bf-ed68-4dc9-b0cc-604434c09a13"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /content/drive/Shareddrives/CIS-5300_Final-Project/glove.840B.300d.zip\n","  inflating: glove.840B.300d.txt     \n","total 5513928\n","drwxr-xr-x 1 root root       4096 Dec  9 20:55 .\n","drwx------ 6 root root       4096 Dec  9 20:55 drive\n","drwxr-xr-x 1 root root       4096 Dec  9 20:55 ..\n","drwxr-xr-x 1 root root       4096 Dec  8 14:36 sample_data\n","drwxr-xr-x 4 root root       4096 Dec  8 14:35 .config\n","-rw-rw-r-- 1 root root 5646236541 Oct 24  2015 glove.840B.300d.txt\n"]}],"source":["#this takes about 10 minutes to run\n","#!wget -nc https://downloads.cs.stanford.edu/nlp/data/glove.840B.300d.zip\n","!unzip /content/drive/Shareddrives/CIS-5300_Final-Project/glove.840B.300d.zip\n","!ls -lat"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"gukyScOlS6Ri","executionInfo":{"status":"ok","timestamp":1670619430307,"user_tz":300,"elapsed":5,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["glove_file = \"glove.840B.300d.txt\""]},{"cell_type":"markdown","source":["## Preprocess"],"metadata":{"id":"k_ZyO9a9QwKg"}},{"cell_type":"code","source":["df_all = pd.concat([df_train, df_validation, df_test]).reset_index()\n","\n","tokenized_data = [word_tokenize(df_all['text'][i]) for i in range(len(df_all['text']))]\n","\n","vocab = {word for sentence in tokenized_data for word in sentence}\n","vocab.add('<PAD>')\n","\n","word_to_idx = { w : i for i, w in enumerate(vocab) }"],"metadata":{"id":"2AsW9_eWTtU6","executionInfo":{"status":"ok","timestamp":1670619431998,"user_tz":300,"elapsed":1694,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","execution_count":9,"metadata":{"id":"fIOXBFqUr1XD","executionInfo":{"status":"ok","timestamp":1670619431999,"user_tz":300,"elapsed":3,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["def pre_process(data, word_to_idx):\n","  tokenized_data = [word_tokenize(data['text'][i]) for i in range(len(data))]\n","\n","  lens = np.array([len(sentence) for sentence in tokenized_data])\n","  \n","  tokens = [word_to_idx[word] for sentence in tokenized_data for word in sentence]\n","\n","  padded_tokens = np.full([len(tokenized_data), max(lens)], word_to_idx['<PAD>'])\n","  for i in range(len(tokenized_data)):\n","    for j in range(len(tokenized_data[i])):\n","      padded_tokens[i][j] = word_to_idx[tokenized_data[i][j]]\n","\n","  labels = np.array(data['label'])\n","    \n","  return padded_tokens, lens, labels"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"oSIxQZurRAzb","executionInfo":{"status":"ok","timestamp":1670619433744,"user_tz":300,"elapsed":1747,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["padded_tokens, lens, labels = pre_process(df_train, word_to_idx)"]},{"cell_type":"markdown","metadata":{"id":"JFe9SkOcYugK"},"source":["## Get Glove Embeddings"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"4RNiORwiYTzi","executionInfo":{"status":"ok","timestamp":1670619433745,"user_tz":300,"elapsed":6,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["#takes about 1 minute to read through the whole file and find the words we need. \n","def get_glove_mapping(vocab, file):\n","    \"\"\"\n","    Gets the mapping of words from the vocabulary to pretrained embeddings\n","    \n","    INPUT:\n","    vocab       - set of vocabulary words\n","    file        - file with pretrained embeddings\n","\n","    OUTPUT:\n","    glove_map   - mapping of words in the vocabulary to the pretrained embedding\n","    \n","    \"\"\"\n","    \n","    glove_map = {}\n","    with open(file,'rb') as fi:\n","        for l in fi:\n","            try:\n","                #### STUDENT CODE HERE ####\n","                emd_lst = l.decode().split(' ')\n","                word = emd_lst.pop(0)\n","                emd_lst = [float(n) for n in emd_lst]\n","\n","                if word in vocab:\n","                  glove_map[word] = np.array(emd_lst)\n","\n","                #### STUDENT CODE ENDS HERE ####\n","            except:\n","                #some lines have urls, we don't need them.\n","                pass\n","    return glove_map"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"kYettRoymWTz","executionInfo":{"status":"ok","timestamp":1670619585459,"user_tz":300,"elapsed":151718,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["glove_map = get_glove_mapping(vocab,glove_file)"]},{"cell_type":"markdown","metadata":{"id":"HoZuTVB9mcZb"},"source":["## Get Embedding Matrix"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"djhLc9X_Z2E0","executionInfo":{"status":"ok","timestamp":1670619585460,"user_tz":300,"elapsed":11,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["def get_dimensions():\n","    d_out =  60 #number of outputs\n","    n_embed =  len(vocab) #size of the dictionary of embeddings\n","    d_embed =  300 # the size of each embedding vector\n","    return d_out, n_embed, d_embed\n","d_out,n_embed,d_embed = get_dimensions()"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"MMIZjt4HYuJB","executionInfo":{"status":"ok","timestamp":1670619585461,"user_tz":300,"elapsed":10,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["def get_embedding_matrix(n_embed, d_embed, glove_map):\n","    \"\"\"\n","    Initialize the weight matrix\n","    \n","    INPUT:\n","    n_embed         - size of the dictionary of embeddings\n","    d_embed         - the size of each embedding vector\n","\n","    OUTPUT:\n","    embedding_matrix  - matrix of mapping from word id to embedding \n","    \"\"\"\n","    #### STUDENT CODE HERE ####\n","    train_words = vocab\n","    embedding_matrix = np.full((n_embed, d_embed), np.random.normal())\n","\n","    for i, word in enumerate(train_words):\n","\n","        if word in glove_map.keys():\n","\n","            embedding_matrix[i] = glove_map[word]\n","    \n","    #### STUDENT CODE ENDS HERE ####\n","    return embedding_matrix"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"rotHEiIqm2sV","executionInfo":{"status":"ok","timestamp":1670619585463,"user_tz":300,"elapsed":11,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["embedding_matrix = get_embedding_matrix(n_embed, d_embed, glove_map)\n","embedding_data = (embedding_matrix.shape, embedding_matrix[:155])"]},{"cell_type":"markdown","metadata":{"id":"Z6rGLI6jm-0g"},"source":["## Define Embedding Layer"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"4XGB1pAabQAf","executionInfo":{"status":"ok","timestamp":1670619585632,"user_tz":300,"elapsed":179,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["def create_emb_layer(embedding_matrix, non_trainable=False):\n","    \"\"\"\n","    Create the embedding layer\n","    \n","    INPUT:\n","    embedding_matrix  - matrix of mapping from word id to embedding\n","    non_trainable   - Flag for whether the weight matrix should be trained. \n","                      If it is set to True, don't update the gradients\n","\n","    OUTPUT:\n","    emb_layer       - embedding layer \n","    \n","    \"\"\"\n","    #### STUDENT CODE HERE ####\n","\n","    emb_layer = nn.Embedding.from_pretrained(torch.Tensor(embedding_matrix), padding_idx=140)\n","\n","    #### STUDENT CODE ENDS HERE ####\n","\n","    return emb_layer"]},{"cell_type":"markdown","metadata":{"id":"FI5HX8dGNQyR"},"source":["## Define Dataloader "]},{"cell_type":"code","execution_count":17,"metadata":{"id":"tOubHTzb8CS2","executionInfo":{"status":"ok","timestamp":1670619585633,"user_tz":300,"elapsed":8,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["class SSTpytorchDataset(Dataset):\n","    def __init__(self, dataset, tokens, word_to_idx, word_dropout = 0.3, split='train'):\n","        super(SSTpytorchDataset, self).__init__()\n","        assert split in ['train', 'test', 'dev'], \"Error!\"\n","        self.ds = dataset\n","        self.split = split\n","        self.word_to_idx = word_to_idx\n","        #self.word_dropout = word_dropout\n","        self.data_x, self.data_ns, self.data_y = pre_process(dataset, self.word_to_idx)\n","\n","    def __len__(self):\n","        return self.data_x.shape[0]\n","    \n","    def __getitem__(self, idx):\n","        if torch.is_tensor(idx):\n","            idx = idx.tolist()\n","        \n","        y = self.data_y[idx] \n","\n","        return self.data_x[idx], self.data_ns[idx], y\n","        "]},{"cell_type":"markdown","metadata":{"id":"nnhB6TKJprvY"},"source":["## Define Train & Evaluate Functions"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"iMPfgoTym0Yj","executionInfo":{"status":"ok","timestamp":1670619585634,"user_tz":300,"elapsed":8,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["def train(model, word_to_idx, lr = .005, drop_out = 0, word_dropout = .3, batch_size = 16, weight_decay = 1e-5, model_type= \"LSTM\"):\n","    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","    trainset = SSTpytorchDataset(df_train, word_dropout, word_to_idx, 'train')\n","    testset = SSTpytorchDataset(df_test, word_dropout, word_to_idx, 'test')\n","    devset = SSTpytorchDataset(df_validation, word_dropout, word_to_idx, 'dev')\n","\n","    train_iter = DataLoader(trainset, batch_size, shuffle=True, num_workers=0)\n","    test_iter = DataLoader(testset, batch_size, shuffle=False, num_workers=0)\n","    dev_iter = DataLoader(devset, batch_size, shuffle=False, num_workers=0)\n","    \n","    model = model\n","    model.to(device)\n","\n","    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay = weight_decay)\n","    print('check')\n","    acc, val_loss = evaluate(dev_iter, model, device, model_type)\n","    best_acc = acc\n","\n","    print(\n","        'epoch |   %        |  loss  |  avg   |val loss|   acc   |  best  | time | save |')\n","    print(\n","        'val   |            |        |        | {:.4f} | {:.4f} | {:.4f} |      |      |'.format(\n","            val_loss, acc, best_acc))\n","\n","    iterations = 0\n","    last_val_iter = 0\n","    train_loss = 0\n","    start = time.time()\n","    _save_ckp = ''\n","    for epoch in range(epochs):\n","        \n","        n_correct, n_total, train_loss = 0, 0, 0\n","        last_val_iter = 0\n","        \n","        for batch_idx, batch in enumerate(train_iter):\n","            # switch model to training mode, clear gradient accumulators\n","            \n","            model.train()\n","            optimizer.zero_grad()\n","            iterations += 1\n","\n","            data, lens, label = batch\n","            data = data.to(device)\n","            label = label.to(device).long()\n","\n","            answer = model(data, lens)\n","\n","            loss = criterion(answer, label)\n","\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","            print('\\r {:4d} | {:4d}/{} | {:.4f} | {:.4f} |'.format(\n","                epoch, batch_size * (batch_idx + 1), len(trainset), loss.item(),\n","                       train_loss / (iterations - last_val_iter)), end='')\n","\n","            if iterations > 0 and iterations % dev_every == 0:\n","                acc, val_loss= evaluate(dev_iter, model, device, model_type)\n","                if acc > best_acc:\n","                    best_acc = acc\n","                    torch.save(model.state_dict(), save_path)\n","                    _save_ckp = '*'\n","\n","                print(\n","                    ' {:.4f} | {:.4f} | {:.4f} | {:.2f} | {:4s} |'.format(\n","                        val_loss, acc, best_acc, (time.time() - start) / 60,\n","                        _save_ckp))\n","\n","                train_loss = 0\n","                last_val_iter = iterations\n","    model.load_state_dict(torch.load(save_path)) #this will be the best model\n","    test_y_pred = evaluate(dev_iter, model, device, model_type, \"test\")\n","    print(\"\\nValidation Accuracy : \", evaluate(dev_iter,model, device, model_type))\n","    return best_acc, test_y_pred\n"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"qKen8d8knAj8","executionInfo":{"status":"ok","timestamp":1670619585635,"user_tz":300,"elapsed":8,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["def evaluate(loader, model, device, model_type = \"LSTM\", split = \"dev\"):\n","    model.eval()\n","    n_correct, n = 0, 0\n","    losses = []\n","    y_pred = []\n","    labels = []\n","    with torch.no_grad():\n","        for batch_idx, batch in enumerate(loader):\n","            data, lens, label = batch\n","            data = data.to(device)\n","            label = label.to(device).long()\n","            \n","            answer = model(data, lens)\n","            if split != \"test\":\n","                n_correct += (torch.max(answer, 1)[1].view(label.size()) == label).sum().item()\n","                n += answer.shape[0]\n","                loss = criterion(answer, label)\n","                losses.append(loss.data.cpu().numpy())\n","            else:\n","                y_pred.extend(torch.max(answer, 1)[1].view(label.size()).tolist())\n","                labels.extend(label.tolist())\n","    if split != \"test\":\n","        acc = 100. * n_correct / n\n","        loss = np.mean(losses)\n","        return acc, loss\n","    else:\n","        return y_pred\n"]},{"cell_type":"markdown","metadata":{"id":"AAflZLIpNybW"},"source":["## Define LSTM Network"]},{"cell_type":"code","execution_count":20,"metadata":{"id":"1q7oqyrCP8fK","executionInfo":{"status":"ok","timestamp":1670619585635,"user_tz":300,"elapsed":7,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"}}},"outputs":[],"source":["import random as random\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from torch.nn import LSTM, GRU\n","\n","class LSTM_Classifier(nn.Module):\n","\n","    def __init__(self,\n","                 n_embed=20000,\n","                 d_embed=300,\n","                 d_hidden=150,\n","                 d_out=59,\n","                 embeddings=None,\n","                 nl = 2,\n","                 bidirectional = True,\n","                 gru = False\n","                 ):\n","        super(LSTM_Classifier, self).__init__()\n","\n","        self.d_hidden = d_hidden\n","        self.bidrectional = bidirectional\n","        self.num_layers = nl\n","\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","        self.embed = create_emb_layer(embedding_matrix,False)\n","        #### STUDENT CODE STARTS HERE ####\n","        self.lstm = nn.LSTM(d_embed, self.d_hidden, self.num_layers, batch_first=True,\n","                              bidirectional=bidirectional)\n","        \n","        if bidirectional:\n","          self.fc_out = nn.Linear(self.d_hidden*2, d_out)\n","        else:\n","          self.fc_out = nn.Linear(self.d_hidden, d_out)\n","        self.dropout = nn.Dropout(p=0.2)\n","        #### STUDENT CODE ENDS HERE ####\n","\n","    def forward(self, text, seq_lengths):\n","\n","        batch_size = text.size()[0]\n","\n","        #### STUDENT CODE STARTS HERE ####\n","        #Fill the missing pieces for the forward pass\n","        x = text\n","        #print(x)\n","        x = self.embed(x)\n","        x = pack_padded_sequence(x, seq_lengths, batch_first=True, enforce_sorted=False)\n","        \n","        x, _ = self.lstm(x)\n","        x, _ = pad_packed_sequence(x, batch_first=True)\n","\n","        #x = self.dropout(x)\n","        x = self.fc_out(x)\n","        x, _ = torch.max(x, 1)\n","        #### STUDENT CODE ENDS HERE ####\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"JmdzEvd2Nc2M"},"source":["## Training Loop"]},{"cell_type":"code","execution_count":21,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":283799,"status":"ok","timestamp":1670619869427,"user":{"displayName":"Lance Lepelstat","userId":"07954586772696797705"},"user_tz":300},"id":"x7FkX-MDokS7","outputId":"7154d065-0ff1-42fb-ee93-faefac5c127d"},"outputs":[{"output_type":"stream","name":"stdout","text":["check\n","epoch |   %        |  loss  |  avg   |val loss|   acc   |  best  | time | save |\n","val   |            |        |        | 4.0949 | 2.7054 | 2.7054 |      |      |\n","    1 | 1280/11514 | 0.8648 | 0.0650 | 0.7144 | 80.4230 | 80.4230 | 0.52 | *    |\n","    2 | 2560/11514 | 0.3931 | 0.0372 | 0.6456 | 83.2759 | 83.2759 | 1.03 | *    |\n","    3 | 3840/11514 | 0.2460 | 0.0229 | 0.6181 | 85.0959 | 85.0959 | 1.55 | *    |\n","    4 | 5120/11514 | 0.1439 | 0.0126 | 0.6284 | 85.5878 | 85.5878 | 2.07 | *    |\n","    5 | 6400/11514 | 0.0426 | 0.0107 | 0.6428 | 85.9813 | 85.9813 | 2.61 | *    |\n","    6 | 7680/11514 | 0.0600 | 0.0069 | 0.6885 | 85.0959 | 85.9813 | 3.12 | *    |\n","    7 | 8960/11514 | 0.0507 | 0.0067 | 0.7265 | 85.3419 | 85.9813 | 3.62 | *    |\n","    8 | 10240/11514 | 0.0936 | 0.0079 | 0.7439 | 85.0959 | 85.9813 | 4.12 | *    |\n","    9 | 11520/11514 | 0.1080 | 0.0092 | 0.7255 | 85.3910 | 85.9813 | 4.64 | *    |\n","\n","Validation Accuracy :  (85.98130841121495, 0.64278483)\n"]}],"source":["torch.manual_seed(1234)\n","criterion = nn.CrossEntropyLoss()\n","batch_size = 128\n","epochs = 10\n","dev_every = 100\n","lr = 0.01\n","save_path = \"best_model\"\n","drop_out = 0\n","word_dropout = 0\n","weight_decay = 0\n","\n","model = LSTM_Classifier(n_embed=n_embed, d_embed=d_embed, d_hidden=150, d_out=d_out, bidirectional=True)\n","dev_value, test_y_pred = train(model, word_to_idx, lr, drop_out, word_dropout, batch_size, weight_decay, \"LSTM\") "]}],"metadata":{"colab":{"provenance":[{"file_id":"1NtblqGYiClITktOKVNTMH5_YGjjNpq_j","timestamp":1670097881819},{"file_id":"17N0rK0m_b8quTu2j_N-HW2AGWEvkHZD4","timestamp":1666564593460},{"file_id":"13xUhf4GEXjXym5_S3Bbg3Ftg0wdIQC8F","timestamp":1665861302092},{"file_id":"1-ueNZYNsPx-ETZbRhGwPB-0AgVmstY0B","timestamp":1665691607811},{"file_id":"1VmAGfOn2wyK6SPcFKhEAbxsgKjrLysb3","timestamp":1665691364654}],"collapsed_sections":["-4WVrNKQC6nQ","ZBlsjn0KncxL","Q5ulmPzRwr3T","k_ZyO9a9QwKg","JFe9SkOcYugK","HoZuTVB9mcZb","Z6rGLI6jm-0g","FI5HX8dGNQyR","nnhB6TKJprvY","AAflZLIpNybW"]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}